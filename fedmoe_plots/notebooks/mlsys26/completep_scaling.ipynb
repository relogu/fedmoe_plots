{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d955566",
   "metadata": {},
   "source": [
    "# Learning Rate Analysis for Centralized and Federated Experiments\n",
    "\n",
    "This notebook analyzes the impact of learning rates on model performance for both centralized and federated training experiments within the `mlsys26` workspace. The primary goal is to visualize the relationship between the learning rate schedule and the training/validation loss.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The notebook connects to Weights & Biases to perform the following steps:\n",
    "\n",
    "1.  **Fetch and Group Runs**: It fetches all runs containing \"fed\" (federated) or \"cen\" (centralized) in their names. It then intelligently groups individual `wandb` runs into logical experiments. A federated experiment, comprising a server and multiple clients, is treated as a single entity.\n",
    "\n",
    "2.  **Filter Experiments**: It filters the experiments to ensure data quality and relevance. The filtering criteria include:\n",
    "    - **Termination Status**: Only experiments where all associated runs have terminated (e.g., 'finished' or 'crashed') are included.\n",
    "    - **Training Completion**: Ensures that the runs have reached their target number of optimization steps.\n",
    "    - **Configuration Matching**: Filters for specific model and optimizer configurations, such as batch size, FFN type, and initial learning rate.\n",
    "\n",
    "3.  **Process Data**: For each logical experiment, it downloads the relevant metrics. To improve performance, it **caches the full history** of each run locally. For federated runs, it aggregates data from all client runs by averaging their metrics at each step.\n",
    "\n",
    "4.  **Generate Plots**: The notebook produces a **dynamic grid of plots** where each subplot corresponds to a specific model configuration (batch size and FFN type). Within each plot, it visualizes:\n",
    "    - **Loss vs. Steps**: The training or validation loss curve across optimization steps.\n",
    "    - **Learning Rate vs. Steps**: The learning rate schedule on a secondary y-axis, providing context for changes in the loss curve.\n",
    "    - Each line in a plot represents a run with a different initial learning rate, allowing for direct comparison of optimizer performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import logging\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from matplotlib.ticker import FixedLocator, FuncFormatter\n",
    "from tqdm.notebook import tqdm\n",
    "from wandb.errors import Error as WandbError\n",
    "\n",
    "from fedmoe_plots.wandb_utils import download_wandb_whole_history\n",
    "\n",
    "\n",
    "def configure_logging_for_jupyter() -> None:\n",
    "    \"\"\"Configure the root logger for clear output in Jupyter notebooks.\"\"\"\n",
    "    root_logger = logging.getLogger()\n",
    "    if root_logger.hasHandlers():\n",
    "        root_logger.handlers.clear()\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    root_logger.addHandler(handler)\n",
    "    root_logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "configure_logging_for_jupyter()\n",
    "log = logging.getLogger(\"lr_analysis.ipynb\")\n",
    "\n",
    "# --- W&B Configuration ---\n",
    "WANDB_ENTITY = \"camlsys\"\n",
    "WANDB_PROJECT = \"mlsys26\"\n",
    "CACHE_DIR = Path(\".cache/wandb_history\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEFAULT_GROUPING_CONFIG_KEYS: dict[str, str] = {\n",
    "    \"batch_size\": \"llm_config.global_train_batch_size\",\n",
    "    \"ffn_type\": \"llm_config.model.ffn_config.ffn_type\",\n",
    "    \"local_steps\": \"fl.n_local_steps\",\n",
    "    \"learning_rate\": \"llm_config.optimizer.lr\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils_markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nested_value(d: dict[str, Any], key_string: str) -> Any | None:  # noqa: ANN401\n",
    "    \"\"\"Access a nested value using a dot-separated key.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : dict[str, Any]\n",
    "        The dictionary to access.\n",
    "    key_string : str\n",
    "        The dot-separated key string.\n",
    "        The dot-separated key string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any | None\n",
    "        The value if found, otherwise None.\n",
    "\n",
    "    \"\"\"\n",
    "    keys = key_string.split(\".\")\n",
    "    current_value = d\n",
    "    for key in keys:\n",
    "        try:\n",
    "            if isinstance(current_value, dict):\n",
    "                current_value = current_value[key]\n",
    "            elif isinstance(current_value, list | tuple) and key.isdigit():\n",
    "                current_value = current_value[int(key)]\n",
    "            else:\n",
    "                return None\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            return None\n",
    "    return current_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41694ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_cache_path(\n",
    "    run: wandb.apis.public.Run,\n",
    ") -> Path:  # pyright: ignore[reportAttributeAccessIssue]\n",
    "    \"\"\"Generate a cache file path for a W&B run.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    run : wandb.apis.public.Run\n",
    "        The W&B run object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        The path to the cached parquet file.\n",
    "\n",
    "    \"\"\"\n",
    "    run_identifier = f\"{run.entity}_{run.project}_{run.id}\"\n",
    "    run_hash = hashlib.md5(run_identifier.encode()).hexdigest()[:12]  # noqa: S324\n",
    "    return CACHE_DIR / f\"{run.name}_{run_hash}.parquet\"\n",
    "\n",
    "\n",
    "def download_run_with_cache(\n",
    "    run: wandb.apis.public.Run,\n",
    ") -> pd.DataFrame:  # pyright: ignore[reportAttributeAccessIssue]\n",
    "    \"\"\"Download W&B run history with local parquet caching.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    run : wandb.apis.public.Run\n",
    "        The W&B run object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The run history as a DataFrame.\n",
    "\n",
    "    \"\"\"\n",
    "    cache_path = get_run_cache_path(run)\n",
    "    if cache_path.exists():\n",
    "        log.debug(\"Loading cached history for %s from %s\", run.name, cache_path)\n",
    "        return pd.read_parquet(cache_path)\n",
    "    log.debug(\"Downloading history for %s (no cache found)\", run.name)\n",
    "    history_df = download_wandb_whole_history(run)\n",
    "    if not history_df.empty:\n",
    "        # Replace inf/-inf values with NaN for parquet compatibility\n",
    "        history_df = history_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # Handle string representations of special values column by column\n",
    "        for col in history_df.columns:\n",
    "            if history_df[col].dtype == object:  # Only process object (string) columns\n",
    "                # Replace string representations of special values\n",
    "                history_df[col] = history_df[col].replace(\n",
    "                    [\"Infinity\", \"-Infinity\", \"NaN\", \"inf\", \"-inf\", \"nan\"],\n",
    "                    np.nan,\n",
    "                )\n",
    "                # Try to convert to numeric if possible\n",
    "                history_df[col] = pd.to_numeric(  # pyright: ignore[reportCallIssue]\n",
    "                    history_df[col],\n",
    "                    errors=\"ignore\",  # pyright: ignore[reportArgumentType]\n",
    "                )\n",
    "\n",
    "        history_df.to_parquet(cache_path, index=False)\n",
    "        log.debug(\"Saved cache for %s to %s\", run.name, cache_path)\n",
    "    return history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_fetching_markdown",
   "metadata": {},
   "source": [
    "## Data Fetching and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77787a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FED_EXPERIMENT_TYPES = {\"fedavg\", \"localadamw\", \"nesterov\"}\n",
    "\n",
    "\n",
    "def get_experiment_type_from_config(config: dict[str, Any]) -> str:\n",
    "    \"\"\"Determine the experiment type from the configuration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict[str, Any]\n",
    "        The configuration dictionary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The experiment type: \"centralized\", \"nesterov\", \"localadamw\", or \"fedavg\".\n",
    "\n",
    "    \"\"\"\n",
    "    run_uuid = get_nested_value(config, \"run_uuid\")\n",
    "    assert run_uuid is not None, \"run_uuid not found in config\"\n",
    "    assert isinstance(\n",
    "        run_uuid,\n",
    "        str,\n",
    "    ), f\"run_uuid should be a string not a {type(run_uuid)}\"\n",
    "    if \"cen\" in run_uuid:\n",
    "        return \"centralized\"\n",
    "    strategy_name = get_nested_value(config, \"fl.strategy_name\")\n",
    "    if strategy_name == \"NESTEROV\":\n",
    "        return \"nesterov\"\n",
    "    sync_states = get_nested_value(config, \"fl.parameter_scheduler_kwargs\")\n",
    "    assert sync_states is not None, \"fl.parameter_scheduler_kwargs not found in config\"\n",
    "    assert isinstance(\n",
    "        sync_states,\n",
    "        dict,\n",
    "    ), f\"fl.parameter_scheduler_kwargs should be a dict not a {type(sync_states)}\"\n",
    "    if len(sync_states) > 1:\n",
    "        return \"localadamw\"\n",
    "    return \"fedavg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_group_experiments(entity: str, project: str) -> list[dict[str, Any]]:\n",
    "    \"\"\"Fetch runs from W&B and group them into logical experiments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entity : str\n",
    "        The W&B entity (user or team).\n",
    "    project : str\n",
    "        The W&B project name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, Any]]\n",
    "        A list of logical experiments, each represented as a dictionary with keys\n",
    "\n",
    "    \"\"\"\n",
    "    log.error(\"🔄 Connecting to W&B and fetching runs from '%s/%s'.\", entity, project)\n",
    "    api = wandb.Api()\n",
    "    try:\n",
    "        runs = api.runs(\n",
    "            path=f\"{entity}/{project}\",\n",
    "            filters={\"display_name\": {\"$regex\": \".*(fed|cen).*\"}},\n",
    "        )\n",
    "    except WandbError:\n",
    "        log.exception(\"❌ Failed to fetch runs from W&B for %s/%s\", entity, project)\n",
    "        return []\n",
    "\n",
    "    log.error(\"Found %d total runs. Grouping into logical experiments.\", len(runs))\n",
    "\n",
    "    grouped_runs = defaultdict(list)\n",
    "    for run in tqdm(runs, desc=\"Grouping Runs\"):\n",
    "        base_name_match = re.match(r\"^(.*?)(_server|_client_\\d+)?$\", run.name)\n",
    "        if base_name_match:\n",
    "            grouped_runs[base_name_match.group(1)].append(run)\n",
    "\n",
    "    experiments = []\n",
    "    for name, run_list in grouped_runs.items():\n",
    "        first_run = run_list[0]\n",
    "        exp_type = get_experiment_type_from_config(first_run.config)\n",
    "        experiments.append({\"name\": name, \"type\": exp_type, \"runs\": run_list})\n",
    "\n",
    "    log.error(\"✅ Grouped into %d logical experiments.\", len(experiments))\n",
    "    # Print all the run uuids\n",
    "    for exp in experiments:\n",
    "        run_uuids = [run.id for run in exp[\"runs\"]]\n",
    "        log.error(\"Experiment '%s' (%s): Runs: %s\", exp[\"name\"], exp[\"type\"], run_uuids)\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_experiments(  # noqa: C901\n",
    "    experiments: list[dict[str, Any]],\n",
    "    config_filters: dict[str, Any],\n",
    "    target_steps_key: str = \"llm_config.scheduler.schedulers.lr.t_max\",\n",
    "    tolerance_steps: int = 1,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Filter a list of logical experiments based on status, completion, and config.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiments : list[dict[str, Any]]\n",
    "        The list of logical experiments to filter.\n",
    "    config_filters : dict[str, Any]\n",
    "        Configuration key-value pairs that each experiment must match.\n",
    "    target_steps_key : str, optional\n",
    "        The config key indicating the target number of training steps,\n",
    "        by default \"llm_config.scheduler.schedulers.lr.t_max\".\n",
    "    tolerance_steps : int, optional\n",
    "        The number of steps below the target to still consider as complete,\n",
    "        by default 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, Any]]\n",
    "        The filtered list of experiments that meet all criteria.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the target_steps_key is not found or\n",
    "        cannot be converted to an integer in the config.\n",
    "\n",
    "    \"\"\"\n",
    "    filtered_list: list[dict[str, Any]] = []\n",
    "    log.error(\n",
    "        \"🔍 Filtering %d experiments with %d config filters (tolerance=%d steps).\",\n",
    "        len(experiments),\n",
    "        len(config_filters),\n",
    "        tolerance_steps,\n",
    "    )\n",
    "\n",
    "    for exp in tqdm(experiments, desc=\"Filtering Experiments\"):\n",
    "        exp_name = exp.get(\"name\", \"unknown\")\n",
    "        runs = exp.get(\"runs\", [])\n",
    "        if not runs:\n",
    "            log.error(\"Skipping %s: no runs present.\", exp_name)\n",
    "            continue\n",
    "\n",
    "        is_terminated = all(r.state in {\"finished\", \"crashed\", \"failed\"} for r in runs)\n",
    "        if not is_terminated:\n",
    "            states = [r.state for r in runs]\n",
    "            log.error(\"Skipping %s: runs not terminated (states=%s).\", exp_name, states)\n",
    "            continue\n",
    "\n",
    "        sample_run = runs[0]\n",
    "        config_match = all(\n",
    "            get_nested_value(sample_run.config, key) == val\n",
    "            for key, val in config_filters.items()\n",
    "        )\n",
    "        if not config_match:\n",
    "            log.error(\"Skipping %s: config mismatch.\", exp_name)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            target_steps_val = get_nested_value(sample_run.config, target_steps_key)\n",
    "            target_steps = (\n",
    "                int(target_steps_val.strip(\"ba\"))\n",
    "                if isinstance(target_steps_val, str)\n",
    "                else None\n",
    "            )\n",
    "            if target_steps is None:\n",
    "                raise ValueError  # noqa: TRY301\n",
    "        except (ValueError, AttributeError):\n",
    "            log.exception(\n",
    "                \"Skipping %s: invalid or missing target steps key '%s'.\",\n",
    "                exp_name,\n",
    "                target_steps_key,\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        runs_to_check = (\n",
    "            [r for r in runs if \"_client_\" in r.name]\n",
    "            if exp[\"type\"] in FED_EXPERIMENT_TYPES\n",
    "            else runs\n",
    "        )\n",
    "        if not runs_to_check:\n",
    "            log.error(\"Skipping %s: no client runs to check for completion.\", exp_name)\n",
    "            continue\n",
    "\n",
    "        def _meets_tolerance(step_value: int, target_steps: int) -> bool:\n",
    "            return (\n",
    "                step_value + tolerance_steps >= target_steps\n",
    "            )  # pyright: ignore[reportOperatorIssue]\n",
    "\n",
    "        is_complete = all(\n",
    "            _meets_tolerance(r.summary.get(\"_step\", 0), target_steps)\n",
    "            for r in runs_to_check\n",
    "        )\n",
    "        if not is_complete:\n",
    "            max_step = max(\n",
    "                (r.summary.get(\"_step\", 0) for r in runs_to_check),\n",
    "                default=0,\n",
    "            )\n",
    "            log.error(\n",
    "                \"Skipping %s: max step %s < target %s within tolerance %s.\",\n",
    "                exp_name,\n",
    "                max_step,\n",
    "                target_steps,\n",
    "                tolerance_steps,\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        filtered_list.append(exp)\n",
    "\n",
    "    log.error(\"✅ Found %d experiments matching all criteria.\", len(filtered_list))\n",
    "    # print all the experiments that match criteria\n",
    "    for exp in filtered_list:\n",
    "        log.error(\"Included Experiment: '%s' (%s)\", exp[\"name\"], exp[\"type\"])\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6g7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_experiments(  # noqa: C901\n",
    "    experiments: list[dict[str, Any]],\n",
    "    metrics: list[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Download/cache metrics and aggregates federated client runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiments : list[dict[str, Any]]\n",
    "        The list of logical experiments to process.\n",
    "    metrics : list[str]\n",
    "        The list of metric names to extract from each experiment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame where each row corresponds to an experiment with its\n",
    "        name, type, config, and processed metric data.\n",
    "\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    log.error(\"📊 Processing %d experiments to get data.\", len(experiments))\n",
    "    columns_to_keep = [\"_step\", *metrics]\n",
    "\n",
    "    def _select_relevant_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "            return pd.DataFrame()\n",
    "        available_cols = [col for col in columns_to_keep if col in df.columns]\n",
    "        missing_cols = [col for col in columns_to_keep if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            log.error(\"History missing columns %s\", missing_cols)\n",
    "        if \"_step\" not in available_cols or len(available_cols) <= 1:\n",
    "            return pd.DataFrame()\n",
    "        return df[available_cols].dropna()\n",
    "\n",
    "    for exp in tqdm(experiments, desc=\"Processing Data\"):\n",
    "        try:\n",
    "            if exp[\"type\"] == \"centralized\":\n",
    "                run = exp[\"runs\"][0]\n",
    "                history = download_run_with_cache(run)\n",
    "                processed_df = _select_relevant_columns(history)\n",
    "            else:  # Federated\n",
    "                client_runs = [r for r in exp[\"runs\"] if \"_client_\" in r.name]\n",
    "                if not client_runs:\n",
    "                    log.warning(\n",
    "                        \"⚠️ Skipping federated experiment %s : no client runs.\",\n",
    "                        exp[\"name\"],\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                client_dfs = []\n",
    "                for cr in client_runs:\n",
    "                    history = download_run_with_cache(cr)\n",
    "                    filtered_history = _select_relevant_columns(history)\n",
    "                    if not filtered_history.empty:\n",
    "                        client_dfs.append(filtered_history)\n",
    "                client_dfs = [df for df in client_dfs if not df.empty]\n",
    "\n",
    "                if not client_dfs:\n",
    "                    log.warning(\n",
    "                        \"⚠️ Skipping %s: no valid client history found.\",\n",
    "                        exp[\"name\"],\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                full_df = pd.concat(client_dfs)\n",
    "                processed_df = full_df.groupby(\"_step\").mean().reset_index()\n",
    "\n",
    "            if processed_df.empty:\n",
    "                log.warning(\"⚠️ Skipping %s: processed data is empty.\", exp[\"name\"])\n",
    "                continue\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"name\": exp[\"name\"],\n",
    "                    \"type\": exp[\"type\"],\n",
    "                    \"config\": exp[\"runs\"][0].config,\n",
    "                    \"data\": processed_df,\n",
    "                },\n",
    "            )\n",
    "        except Exception as _:\n",
    "            log.exception(\"❌ Error processing %s\", exp[\"name\"])\n",
    "\n",
    "    log.error(\"✅ Successfully processed %d experiments.\", len(results))\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plotting_markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(  # noqa: C901, PLR0912, PLR0914, PLR0915\n",
    "    results_df: pd.DataFrame,\n",
    "    loss_metric_key: str,\n",
    "    y_limits: tuple[float, float] | None = None,\n",
    "    grouping_config_keys: dict[str, str] | None = None,\n",
    "    lr_sig_digits: int = 2,  # noqa: ARG001\n",
    ") -> None:\n",
    "    \"\"\"Aggregate final metrics per run and plots learning rate sweeps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame with experiment results, each row containing 'name', 'type',\n",
    "        'config', and 'data' (DataFrame of metrics).\n",
    "    loss_metric_key : str\n",
    "        The key of the loss metric to analyze (e.g., \"eval/loss\").\n",
    "    y_limits : tuple[float, float] | None, optional\n",
    "        Y-axis limits for the plots, by default None.\n",
    "    grouping_config_keys : dict[str, str] | None, optional\n",
    "        Mapping of config aliases to their dot-separated paths in the config.\n",
    "        Must include \"learning_rate\"\n",
    "        By default None, which uses DEFAULT_GROUPING_CONFIG_KEYS.\n",
    "    lr_sig_digits : int, optional\n",
    "        Number of significant digits to display for learning rate axis values,\n",
    "        by default 2.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If 'learning_rate' is not included in grouping_config_keys.\n",
    "\n",
    "    \"\"\"\n",
    "    if results_df.empty:\n",
    "        log.warning(\"⚠️ Results DataFrame is empty. Cannot generate plots.\")\n",
    "        return\n",
    "\n",
    "    if grouping_config_keys is None:\n",
    "        grouping_config_keys = DEFAULT_GROUPING_CONFIG_KEYS.copy()\n",
    "    if \"learning_rate\" not in grouping_config_keys:\n",
    "        msg = \"grouping_config_keys must include a 'learning_rate' entry.\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    panel_group_columns: list[str] = [\"experiment_type\"]\n",
    "    for alias in grouping_config_keys:\n",
    "        if alias != \"learning_rate\" and alias not in panel_group_columns:\n",
    "            panel_group_columns.append(alias)\n",
    "\n",
    "    def _extract_final_metric(df: pd.DataFrame) -> float | None:\n",
    "        if not isinstance(df, pd.DataFrame) or loss_metric_key not in df.columns:\n",
    "            return None\n",
    "        series = df[loss_metric_key].dropna()\n",
    "        if series.empty:\n",
    "            return None\n",
    "        final_value = series.iloc[-1]\n",
    "        if not np.isfinite(final_value):\n",
    "            return None\n",
    "        return float(final_value)\n",
    "\n",
    "    def _normalize_config_value(  # noqa: C901, PLR0911\n",
    "        alias: str,\n",
    "        raw_value: Any,  # noqa: ANN401\n",
    "    ) -> Any | None:  # noqa: ANN401\n",
    "        if raw_value is None:\n",
    "            return None\n",
    "        if alias == \"learning_rate\":\n",
    "            try:\n",
    "                return float(raw_value)\n",
    "            except (TypeError, ValueError):\n",
    "                return None\n",
    "        if isinstance(raw_value, int | np.integer):\n",
    "            return int(raw_value)\n",
    "        if isinstance(raw_value, float | np.floating):\n",
    "            return float(raw_value)\n",
    "        if isinstance(raw_value, str):\n",
    "            stripped = raw_value.strip()\n",
    "            if alias in {\"batch_size\", \"local_steps\"} and stripped.isdigit():\n",
    "                return int(stripped)\n",
    "            try:\n",
    "                numeric_val = float(stripped)\n",
    "                if numeric_val.is_integer():\n",
    "                    return int(numeric_val)\n",
    "                return numeric_val  # noqa: TRY300\n",
    "            except ValueError:\n",
    "                return stripped\n",
    "        if isinstance(raw_value, bool):\n",
    "            return raw_value\n",
    "        return str(raw_value)\n",
    "\n",
    "    def _round_to_n_significant(value: float, n_sig: int = 2) -> float:\n",
    "        \"\"\"Round a value to N significant digits.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : float\n",
    "            The value to round.\n",
    "        n_sig : int, optional\n",
    "            Number of significant digits, by default 2.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The rounded value.\n",
    "\n",
    "        \"\"\"\n",
    "        if value == 0.0 or not math.isfinite(value):\n",
    "            return value\n",
    "        magnitude = math.floor(math.log10(abs(value)))\n",
    "        return round(value, -magnitude + n_sig - 1)\n",
    "\n",
    "    def _apply_scientific_offset(\n",
    "        axis: Any,  # noqa: ANN401\n",
    "        values: np.ndarray,\n",
    "        n_sig_digits: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Apply scientific offset formatting to axis with scale shown on the axis.\"\"\"\n",
    "        finite_vals: list[float] = []\n",
    "        for raw_val in values:\n",
    "            if not isinstance(raw_val, int | float | np.floating):\n",
    "                continue\n",
    "            val = float(raw_val)\n",
    "            if not math.isfinite(val) or val == 0.0:\n",
    "                continue\n",
    "            finite_vals.append(val)\n",
    "        if not finite_vals:\n",
    "            axis.set_major_formatter(FuncFormatter(lambda val, _pos: f\"{val:.4g}\"))\n",
    "            return\n",
    "        max_abs = max(abs(v) for v in finite_vals)\n",
    "        if max_abs == 0.0:\n",
    "            axis.set_major_formatter(FuncFormatter(lambda _val, _pos: \"0\"))\n",
    "            return\n",
    "        exponent = math.floor(math.log10(max_abs))\n",
    "        if exponent == 0 or abs(exponent) <= 2:\n",
    "            # For small exponents, show the numbers directly without offset\n",
    "            axis.set_major_formatter(\n",
    "                FuncFormatter(\n",
    "                    lambda val, _pos: f\"{_round_to_n_significant(val, n_sig_digits)}\",\n",
    "                ),\n",
    "            )\n",
    "            return\n",
    "        scale = 10**exponent\n",
    "        formatter = FuncFormatter(\n",
    "            lambda val, _pos: (\n",
    "                \"0\"\n",
    "                if val == 0\n",
    "                else f\"{_round_to_n_significant(val / scale, n_sig_digits)}\"\n",
    "            ),\n",
    "        )\n",
    "        axis.set_major_formatter(formatter)\n",
    "        # Set the offset text to display the scale at the end of the axis\n",
    "        # Get the axis object to set the label\n",
    "        ax = axis.axes\n",
    "        current_label = ax.get_xlabel() if axis == ax.xaxis else ax.get_ylabel()\n",
    "        if current_label:\n",
    "            # Append the scale to the existing label\n",
    "            new_label = rf\"{current_label} ($\\times 10^{{{exponent}}}$)\"\n",
    "            if axis == ax.xaxis:\n",
    "                ax.set_xlabel(new_label, fontsize=12)\n",
    "            else:\n",
    "                ax.set_ylabel(new_label, fontsize=12)\n",
    "\n",
    "    summary_records: list[dict[str, Any]] = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        final_metric = _extract_final_metric(row[\"data\"])\n",
    "        if final_metric is None:\n",
    "            log.error(\"Skipping %s: missing %s values\", row[\"name\"], loss_metric_key)\n",
    "            continue\n",
    "\n",
    "        config = row[\"config\"]\n",
    "        config_values: dict[str, Any] = {}\n",
    "        missing_required = False\n",
    "        missing_optional: list[str] = []\n",
    "        for alias, path in grouping_config_keys.items():\n",
    "            raw_value = get_nested_value(config, path)\n",
    "            normalized_value = _normalize_config_value(alias, raw_value)\n",
    "            if normalized_value is None:\n",
    "                if alias == \"learning_rate\":\n",
    "                    missing_required = True\n",
    "                    break\n",
    "                missing_optional.append(alias)\n",
    "            config_values[alias] = normalized_value\n",
    "        if missing_required:\n",
    "            log.error(\n",
    "                \"Skipping %s: missing learning_rate config for aggregation\",\n",
    "                row[\"name\"],\n",
    "            )\n",
    "            continue\n",
    "        for alias in missing_optional:\n",
    "            config_values[alias] = \"unknown\"\n",
    "\n",
    "        summary_records.append(\n",
    "            {\n",
    "                \"experiment_type\": row[\"type\"],\n",
    "                **config_values,\n",
    "                \"final_metric\": final_metric,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_records)\n",
    "    if summary_df.empty:\n",
    "        log.warning(\"⚠️ No runs with valid %s data.\", loss_metric_key)\n",
    "        return\n",
    "\n",
    "    group_columns = panel_group_columns.copy()\n",
    "    if \"learning_rate\" not in group_columns:\n",
    "        group_columns.append(\"learning_rate\")\n",
    "\n",
    "    aggregated = (\n",
    "        summary_df.groupby(group_columns, dropna=False)\n",
    "        .agg(\n",
    "            mean_metric=(\"final_metric\", \"mean\"),\n",
    "            std_metric=(\"final_metric\", \"std\"),\n",
    "            n_runs=(\"final_metric\", \"size\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    aggregated[\"std_metric\"] = aggregated[\"std_metric\"].fillna(0.0)\n",
    "\n",
    "    grouped_panels = list(aggregated.groupby(panel_group_columns, dropna=False))\n",
    "    if not grouped_panels:\n",
    "        log.warning(\"⚠️ No grouped configurations to visualize.\")\n",
    "        return\n",
    "\n",
    "    n_panels = len(grouped_panels)\n",
    "    n_cols = min(3, max(1, n_panels))\n",
    "    n_rows = math.ceil(n_panels / n_cols)\n",
    "    fig, axes = plt.subplots(\n",
    "        n_rows,\n",
    "        n_cols,\n",
    "        figsize=(6 * n_cols, 4.5 * n_rows),\n",
    "        squeeze=False,\n",
    "    )\n",
    "    axes_flat = axes.flatten()\n",
    "    metric_label = loss_metric_key.rsplit(\"/\", maxsplit=1)[-1]\n",
    "\n",
    "    for idx, (panel_key, panel_df) in enumerate(grouped_panels):\n",
    "        ax = axes_flat[idx]\n",
    "        if len(panel_group_columns) == 1:\n",
    "            key_values = {panel_group_columns[0]: panel_key}\n",
    "        else:\n",
    "            key_values = dict(zip(panel_group_columns, panel_key, strict=False))\n",
    "        ordered_series = panel_df.sort_values(\"learning_rate\")\n",
    "        lr_values = np.sort(ordered_series[\"learning_rate\"].astype(float).unique())\n",
    "        exp_type_value = key_values.get(\n",
    "            \"experiment_type\",\n",
    "            panel_df[\"experiment_type\"].iloc[0],\n",
    "        )\n",
    "        run_count = int(panel_df[\"n_runs\"].sum())\n",
    "        label = f\"{str(exp_type_value).title()} (n={run_count})\"\n",
    "        ax.errorbar(\n",
    "            ordered_series[\"learning_rate\"],\n",
    "            ordered_series[\"mean_metric\"],\n",
    "            yerr=ordered_series[\"std_metric\"],\n",
    "            fmt=\"o-\",\n",
    "            color=plt.cm.tab10(idx % 10),  # pyright: ignore[reportAttributeAccessIssue]\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            capsize=4,\n",
    "            label=label,\n",
    "        )\n",
    "\n",
    "        ax.set_ylabel(metric_label, fontsize=12)\n",
    "        ax.set_xlabel(\"Learning Rate\", fontsize=12)\n",
    "        ax.set_xscale(\"log\")\n",
    "\n",
    "        # Set up custom tick system:\n",
    "        # - Major ticks at integer powers (10^-3, 10^-2) AND\n",
    "        #   half-powers (10^-2.5, 10^-1.5)  # noqa: ERA001\n",
    "        # - Minor ticks at quarter-powers (10^-2.75, 10^-2.25, etc.) without labels\n",
    "\n",
    "        # Determine the range for ticks\n",
    "        lr_min, lr_max = lr_values.min(), lr_values.max()\n",
    "        log_min = np.floor(np.log10(lr_min))\n",
    "        log_max = np.ceil(np.log10(lr_max))\n",
    "\n",
    "        # Generate major tick positions: integer and half powers\n",
    "        major_ticks = []\n",
    "        for n in range(int(log_min) - 1, int(log_max) + 2):\n",
    "            major_ticks.extend((10**n, 10 ** (n + 0.5)))\n",
    "\n",
    "        # Generate minor tick positions: quarter powers (0.25 and 0.75)\n",
    "        minor_ticks = []\n",
    "        for n in range(int(log_min) - 1, int(log_max) + 2):\n",
    "            minor_ticks.extend((10 ** (n + 0.25), 10 ** (n + 0.75)))\n",
    "\n",
    "        # Set the locators\n",
    "        ax.xaxis.set_major_locator(FixedLocator(major_ticks))\n",
    "        ax.xaxis.set_minor_locator(FixedLocator(minor_ticks))\n",
    "\n",
    "        # Format major ticks to show 10^n notation\n",
    "        def format_major_tick(x: float, _p: int) -> str:\n",
    "            if x <= 0:\n",
    "                return \"\"\n",
    "            exponent = np.log10(x)\n",
    "            # Check if this is close to an integer or half-integer power\n",
    "            if abs(exponent - round(exponent)) < 0.01:\n",
    "                # Integer power\n",
    "                return f\"$10^{{{round(exponent):.0f}}}$\"\n",
    "            if abs(exponent - round(exponent * 2) / 2) < 0.01:\n",
    "                # Half power\n",
    "                return f\"$10^{{{exponent:.1f}}}$\"\n",
    "            return \"\"\n",
    "\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(format_major_tick))\n",
    "\n",
    "        if y_limits is not None:\n",
    "            ax.set_ylim(*y_limits)\n",
    "        ax.grid(True, which=\"major\", linestyle=\"--\", alpha=0.6)  # noqa: FBT003\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        title_parts: list[str] = []\n",
    "        for column in panel_group_columns:\n",
    "            value = key_values.get(column, \"unknown\")\n",
    "            if column == \"experiment_type\":\n",
    "                label_name = \"Type\"\n",
    "            else:\n",
    "                label_name = column.replace(\"_\", \" \").title()\n",
    "            title_parts.append(f\"{label_name}: {value}\")\n",
    "        title_text = \"\\n\".join(title_parts) if title_parts else \"Learning Rate Sweep\"\n",
    "        ax.set_title(title_text, fontsize=13)\n",
    "\n",
    "    for idx in range(n_panels, len(axes_flat)):\n",
    "        axes_flat[idx].set_visible(False)\n",
    "\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "\n",
    "    sort_columns = [col for col in group_columns if col in aggregated.columns]\n",
    "    summary_table = aggregated.sort_values(sort_columns)\n",
    "    log.error(\n",
    "        \"Aggregated LR vs %s table:\\n%s\",\n",
    "        metric_label,\n",
    "        summary_table.to_string(index=False, float_format=\"%.4f\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_execution_markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a599efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparison_plots(  # noqa: C901, PLR0912, PLR0914, PLR0915\n",
    "    results_df: pd.DataFrame,\n",
    "    loss_metric_key: str,\n",
    "    comparison_key: str,\n",
    "    y_limits: tuple[float, float] | None = None,\n",
    "    grouping_config_keys: dict[str, str] | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Generate plots comparing different values of a configuration key.\n",
    "\n",
    "    This function creates plots where each line represents a different value\n",
    "    of the comparison_key (e.g., different FFN types), allowing comparison\n",
    "    across that dimension while grouping by other configuration parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame with experiment results, each row containing 'name', 'type',\n",
    "        'config', and 'data' (DataFrame of metrics).\n",
    "    loss_metric_key : str\n",
    "        The key of the loss metric to analyze (e.g., \"eval/loss\").\n",
    "    comparison_key : str\n",
    "        The configuration key to compare as different lines in each plot\n",
    "        (e.g., \"ffn_type\").\n",
    "    y_limits : tuple[float, float] | None, optional\n",
    "        Y-axis limits for the plots, by default None.\n",
    "    grouping_config_keys : dict[str, str] | None, optional\n",
    "        Mapping of config aliases to their dot-separated paths in the config.\n",
    "        Must include \"learning_rate\" and comparison_key.\n",
    "        By default None, which uses DEFAULT_GROUPING_CONFIG_KEYS.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If 'learning_rate' or comparison_key is not included in grouping_config_keys.\n",
    "\n",
    "    \"\"\"\n",
    "    if results_df.empty:\n",
    "        log.warning(\"⚠️ Results DataFrame is empty. Cannot generate plots.\")\n",
    "        return\n",
    "\n",
    "    if grouping_config_keys is None:\n",
    "        grouping_config_keys = DEFAULT_GROUPING_CONFIG_KEYS.copy()\n",
    "    if \"learning_rate\" not in grouping_config_keys:\n",
    "        msg = \"grouping_config_keys must include a 'learning_rate' entry.\"\n",
    "        raise ValueError(msg)\n",
    "    if comparison_key not in grouping_config_keys:\n",
    "        msg = (\n",
    "            f\"grouping_config_keys must include the comparison_key \"\n",
    "            f\"'{comparison_key}'.\"\n",
    "        )\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Panel groups include everything except learning_rate and comparison_key\n",
    "    panel_group_columns: list[str] = [\"experiment_type\"]\n",
    "    for alias in grouping_config_keys:\n",
    "        if (\n",
    "            alias not in {\"learning_rate\", comparison_key}\n",
    "            and alias not in panel_group_columns\n",
    "        ):\n",
    "            panel_group_columns.append(alias)\n",
    "\n",
    "    def _extract_final_metric(df: pd.DataFrame) -> float | None:\n",
    "        if not isinstance(df, pd.DataFrame) or loss_metric_key not in df.columns:\n",
    "            return None\n",
    "        series = df[loss_metric_key].dropna()\n",
    "        if series.empty:\n",
    "            return None\n",
    "        final_value = series.iloc[-1]\n",
    "        if not np.isfinite(final_value):\n",
    "            return None\n",
    "        return float(final_value)\n",
    "\n",
    "    def _normalize_config_value(  # noqa: C901, PLR0911\n",
    "        alias: str,\n",
    "        raw_value: Any,  # noqa: ANN401\n",
    "    ) -> Any | None:  # noqa: ANN401\n",
    "        if raw_value is None:\n",
    "            return None\n",
    "        if alias == \"learning_rate\":\n",
    "            try:\n",
    "                return float(raw_value)\n",
    "            except (TypeError, ValueError):\n",
    "                return None\n",
    "        if isinstance(raw_value, int | np.integer):\n",
    "            return int(raw_value)\n",
    "        if isinstance(raw_value, float | np.floating):\n",
    "            return float(raw_value)\n",
    "        if isinstance(raw_value, str):\n",
    "            stripped = raw_value.strip()\n",
    "            if alias in {\"batch_size\", \"local_steps\"} and stripped.isdigit():\n",
    "                return int(stripped)\n",
    "            try:\n",
    "                numeric_val = float(stripped)\n",
    "                if numeric_val.is_integer():\n",
    "                    return int(numeric_val)\n",
    "                return numeric_val  # noqa: TRY300\n",
    "            except ValueError:\n",
    "                return stripped\n",
    "        if isinstance(raw_value, bool):\n",
    "            return raw_value\n",
    "        return str(raw_value)\n",
    "\n",
    "    summary_records: list[dict[str, Any]] = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        final_metric = _extract_final_metric(row[\"data\"])\n",
    "        if final_metric is None:\n",
    "            log.error(\"Skipping %s: missing %s values\", row[\"name\"], loss_metric_key)\n",
    "            continue\n",
    "\n",
    "        config = row[\"config\"]\n",
    "        config_values: dict[str, Any] = {}\n",
    "        missing_required = False\n",
    "        missing_optional: list[str] = []\n",
    "        for alias, path in grouping_config_keys.items():\n",
    "            raw_value = get_nested_value(config, path)\n",
    "            normalized_value = _normalize_config_value(alias, raw_value)\n",
    "            if normalized_value is None:\n",
    "                if alias in {\"learning_rate\", comparison_key}:\n",
    "                    missing_required = True\n",
    "                    break\n",
    "                missing_optional.append(alias)\n",
    "            config_values[alias] = normalized_value\n",
    "        if missing_required:\n",
    "            log.error(\n",
    "                \"Skipping %s: missing required config for aggregation\",\n",
    "                row[\"name\"],\n",
    "            )\n",
    "            continue\n",
    "        for alias in missing_optional:\n",
    "            config_values[alias] = \"unknown\"\n",
    "\n",
    "        summary_records.append(\n",
    "            {\n",
    "                \"experiment_type\": row[\"type\"],\n",
    "                **config_values,\n",
    "                \"final_metric\": final_metric,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_records)\n",
    "    if summary_df.empty:\n",
    "        log.warning(\"⚠️ No runs with valid %s data.\", loss_metric_key)\n",
    "        return\n",
    "\n",
    "    group_columns = panel_group_columns.copy()\n",
    "    if \"learning_rate\" not in group_columns:\n",
    "        group_columns.append(\"learning_rate\")\n",
    "    if comparison_key not in group_columns:\n",
    "        group_columns.append(comparison_key)\n",
    "\n",
    "    aggregated = (\n",
    "        summary_df.groupby(group_columns, dropna=False)\n",
    "        .agg(\n",
    "            mean_metric=(\"final_metric\", \"mean\"),\n",
    "            std_metric=(\"final_metric\", \"std\"),\n",
    "            n_runs=(\"final_metric\", \"size\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    aggregated[\"std_metric\"] = aggregated[\"std_metric\"].fillna(0.0)\n",
    "\n",
    "    grouped_panels = list(aggregated.groupby(panel_group_columns, dropna=False))\n",
    "    if not grouped_panels:\n",
    "        log.warning(\"⚠️ No grouped configurations to visualize.\")\n",
    "        return\n",
    "\n",
    "    n_panels = len(grouped_panels)\n",
    "    n_cols = min(3, max(1, n_panels))\n",
    "    n_rows = math.ceil(n_panels / n_cols)\n",
    "    fig, axes = plt.subplots(\n",
    "        n_rows,\n",
    "        n_cols,\n",
    "        figsize=(6 * n_cols, 4.5 * n_rows),\n",
    "        squeeze=False,\n",
    "    )\n",
    "    axes_flat = axes.flatten()\n",
    "    metric_label = loss_metric_key.rsplit(\"/\", maxsplit=1)[-1]\n",
    "\n",
    "    for idx, (panel_key, panel_df) in enumerate(grouped_panels):\n",
    "        ax = axes_flat[idx]\n",
    "        if len(panel_group_columns) == 1:\n",
    "            key_values = {panel_group_columns[0]: panel_key}\n",
    "        else:\n",
    "            key_values = dict(zip(panel_group_columns, panel_key, strict=False))\n",
    "\n",
    "        # Get unique comparison values to plot as separate lines\n",
    "        comparison_values = sorted(panel_df[comparison_key].unique())\n",
    "\n",
    "        for line_idx, comp_val in enumerate(comparison_values):\n",
    "            comp_df = panel_df[panel_df[comparison_key] == comp_val]\n",
    "            ordered_series = comp_df.sort_values(\"learning_rate\")\n",
    "            run_count = int(ordered_series[\"n_runs\"].sum())\n",
    "\n",
    "            label = f\"{comp_val} (n={run_count})\"\n",
    "            ax.errorbar(\n",
    "                ordered_series[\"learning_rate\"],\n",
    "                ordered_series[\"mean_metric\"],\n",
    "                yerr=ordered_series[\"std_metric\"],\n",
    "                fmt=\"o-\",\n",
    "                color=plt.cm.tab10(\n",
    "                    line_idx % 10,\n",
    "                ),  # pyright: ignore[reportAttributeAccessIssue]\n",
    "                linewidth=2,\n",
    "                markersize=6,\n",
    "                capsize=4,\n",
    "                label=label,\n",
    "            )\n",
    "\n",
    "        ax.set_ylabel(metric_label, fontsize=12)\n",
    "        ax.set_xlabel(\"Learning Rate\", fontsize=12)\n",
    "        ax.set_xscale(\"log\")\n",
    "\n",
    "        # Set up custom tick system:\n",
    "        # - Major ticks at integer powers (10^-3, 10^-2) AND\n",
    "        #   half-powers (10^-2.5, 10^-1.5)  # noqa: ERA001\n",
    "        # - Minor ticks at quarter-powers (10^-2.75, 10^-2.25, etc.) without labels\n",
    "\n",
    "        # Determine the range for ticks\n",
    "        panel_lr_values = panel_df[\"learning_rate\"].astype(float).to_numpy()\n",
    "        lr_min, lr_max = panel_lr_values.min(), panel_lr_values.max()\n",
    "        log_min = np.floor(np.log10(lr_min))\n",
    "        log_max = np.ceil(np.log10(lr_max))\n",
    "\n",
    "        # Generate major tick positions: integer and half powers\n",
    "        major_ticks = []\n",
    "        for n in range(int(log_min) - 1, int(log_max) + 2):\n",
    "            major_ticks.extend((10**n, 10 ** (n + 0.5)))\n",
    "\n",
    "        # Generate minor tick positions: quarter powers (0.25 and 0.75)\n",
    "        minor_ticks = []\n",
    "        for n in range(int(log_min) - 1, int(log_max) + 2):\n",
    "            minor_ticks.extend((10 ** (n + 0.25), 10 ** (n + 0.75)))\n",
    "\n",
    "        # Set the locators\n",
    "        ax.xaxis.set_major_locator(FixedLocator(major_ticks))\n",
    "        ax.xaxis.set_minor_locator(FixedLocator(minor_ticks))\n",
    "\n",
    "        # Format major ticks to show 10^n notation\n",
    "        def format_major_tick(x: float, _p: int) -> str:\n",
    "            if x <= 0:\n",
    "                return \"\"\n",
    "            exponent = np.log10(x)\n",
    "            # Check if this is close to an integer or half-integer power\n",
    "            if abs(exponent - round(exponent)) < 0.01:\n",
    "                # Integer power\n",
    "                return f\"$10^{{{round(exponent):.0f}}}$\"\n",
    "            if abs(exponent - round(exponent * 2) / 2) < 0.01:\n",
    "                # Half power\n",
    "                return f\"$10^{{{exponent:.1f}}}$\"\n",
    "            return \"\"\n",
    "\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(format_major_tick))\n",
    "\n",
    "        if y_limits is not None:\n",
    "            ax.set_ylim(*y_limits)\n",
    "        ax.grid(True, which=\"major\", linestyle=\"--\", alpha=0.6)  # noqa: FBT003\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "        # Create title from panel grouping\n",
    "        title_parts: list[str] = []\n",
    "        for column in panel_group_columns:\n",
    "            value = key_values.get(column, \"unknown\")\n",
    "            if column == \"experiment_type\":\n",
    "                label_name = \"Type\"\n",
    "            else:\n",
    "                label_name = column.replace(\"_\", \" \").title()\n",
    "            title_parts.append(f\"{label_name}: {value}\")\n",
    "        comparison_title = comparison_key.replace(\"_\", \" \").title()\n",
    "        title_text = (\n",
    "            \"\\n\".join(title_parts) if title_parts else f\"{comparison_title} Comparison\"\n",
    "        )\n",
    "        ax.set_title(title_text, fontsize=13)\n",
    "\n",
    "    for idx in range(n_panels, len(axes_flat)):\n",
    "        axes_flat[idx].set_visible(False)\n",
    "\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "\n",
    "    sort_columns = [col for col in group_columns if col in aggregated.columns]\n",
    "    summary_table = aggregated.sort_values(sort_columns)\n",
    "    log.error(\n",
    "        \"Aggregated comparison for %s:\\n%s\",\n",
    "        metric_label,\n",
    "        summary_table.to_string(index=False, float_format=\"%.4f\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6g7h8i9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define filters for the experiments.\n",
    "CONFIG_FILTERS = {\n",
    "    # 'llm_config.global_train_batch_size': 512,\n",
    "    # 'llm_config.model.ffn_config.ffn_type': 'mptmlp',\n",
    "}\n",
    "\n",
    "# 2. Choose the loss metric to plot.\n",
    "LOSS_METRIC = \"metrics/train/LanguagePerplexity\"  # Or 'loss/validation'\n",
    "METRICS_TO_DOWNLOAD = [LOSS_METRIC]\n",
    "Y_LIMITS: tuple[float, float] | None = (40, 200)  # Example: (40, 200)\n",
    "GROUPING_CONFIG_KEYS = DEFAULT_GROUPING_CONFIG_KEYS.copy()\n",
    "LR_SIG_DIGITS = 2  # Number of significant digits for learning rate axis\n",
    "\n",
    "# 3. Fetch and group all experiments from W&B.\n",
    "all_experiments = fetch_and_group_experiments(WANDB_ENTITY, WANDB_PROJECT)\n",
    "\n",
    "# 4. Filter experiments based on completion and configuration.\n",
    "if all_experiments:\n",
    "    filtered_experiments = filter_experiments(\n",
    "        all_experiments,\n",
    "        config_filters=CONFIG_FILTERS,\n",
    "    )\n",
    "else:\n",
    "    filtered_experiments = []\n",
    "\n",
    "# 5. Process filtered experiments to get data, using cache if available.\n",
    "if filtered_experiments:\n",
    "    results_df = process_experiments(filtered_experiments, metrics=METRICS_TO_DOWNLOAD)\n",
    "else:\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "# 6. Generate and display the plots.\n",
    "if not results_df.empty:\n",
    "    log.error(\"\\n--- Plot Generation ---\")\n",
    "    generate_plots(\n",
    "        results_df,\n",
    "        loss_metric_key=LOSS_METRIC,\n",
    "        y_limits=Y_LIMITS,\n",
    "        grouping_config_keys=GROUPING_CONFIG_KEYS,\n",
    "        lr_sig_digits=LR_SIG_DIGITS,\n",
    "    )\n",
    "else:\n",
    "    log.warning(\"\\nNo data available to generate plots after filtering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b900070",
   "metadata": {},
   "source": [
    "## Comparison Plots (Optional)\n",
    "\n",
    "Use this section to compare different experiment configurations (e.g., different FFN types) as separate lines in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f654ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare different FFN types on the same plot\n",
    "# Each line will represent a different FFN type (mptmlp vs sigma_moe)\n",
    "# while grouping by batch_size and local_steps\n",
    "\n",
    "if not results_df.empty:\n",
    "    log.error(\"\\n--- Comparison Plot Generation ---\")\n",
    "    generate_comparison_plots(\n",
    "        results_df,\n",
    "        loss_metric_key=LOSS_METRIC,\n",
    "        comparison_key=\"ffn_type\",  # Change this to compare different config keys\n",
    "        y_limits=Y_LIMITS,\n",
    "        grouping_config_keys=GROUPING_CONFIG_KEYS,\n",
    "    )\n",
    "else:\n",
    "    log.warning(\"\\nNo data available to generate comparison plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b823ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare different FFN types on the same plot\n",
    "# Each line will represent a different FFN type (mptmlp vs sigma_moe)\n",
    "# while grouping by batch_size and local_steps\n",
    "\n",
    "if not results_df.empty:\n",
    "    log.error(\"\\n--- Comparison Plot Generation ---\")\n",
    "    generate_comparison_plots(\n",
    "        results_df,\n",
    "        loss_metric_key=LOSS_METRIC,\n",
    "        comparison_key=\"local_steps\",  # Change this to compare different config keys\n",
    "        y_limits=Y_LIMITS,\n",
    "        grouping_config_keys=GROUPING_CONFIG_KEYS,\n",
    "    )\n",
    "else:\n",
    "    log.warning(\"\\nNo data available to generate comparison plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b1c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare different FFN types on the same plot\n",
    "# Each line will represent a different FFN type (mptmlp vs sigma_moe)\n",
    "# while grouping by batch_size and local_steps\n",
    "\n",
    "if not results_df.empty:\n",
    "    log.error(\"\\n--- Comparison Plot Generation ---\")\n",
    "    generate_comparison_plots(\n",
    "        results_df,\n",
    "        loss_metric_key=LOSS_METRIC,\n",
    "        comparison_key=\"batch_size\",  # Change this to compare different config keys\n",
    "        y_limits=Y_LIMITS,\n",
    "        grouping_config_keys=GROUPING_CONFIG_KEYS,\n",
    "    )\n",
    "else:\n",
    "    log.warning(\"\\nNo data available to generate comparison plots.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedmoe-plots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
