{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d955566",
   "metadata": {},
   "source": [
    "# ŒºP Complete-P Scaling Analysis\n",
    "\n",
    "This notebook analyzes the scaling behavior of Mixture of Experts (MoE) models using ŒºP (Maximal Update Parameterization) with Complete-P scaling methodology. The analysis focuses on understanding how different hyperparameter configurations affect model performance across various model sizes and expert configurations.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The notebook performs a comprehensive analysis of training runs from the `camlsys/photon` project, examining:\n",
    "\n",
    "- **Model Architectures**: Dense models (MPT variants) and MoE models with 4 or 8 experts\n",
    "- **Scaling Dimensions**: Width multipliers, depth multipliers, and base model dimensions\n",
    "- **Training Hyperparameters**: Learning rates and batch sizes\n",
    "- **Model Configurations**: Use of Peri-LN, embedding normalization, and bias settings\n",
    "\n",
    "## Key Analysis Components\n",
    "\n",
    "1. **Data Collection**: Downloads training metrics from Weights & Biases for specified model runs\n",
    "2. **Parameter Counting**: Computes total trainable parameters, expert vs non-expert parameters\n",
    "3. **Performance Evaluation**: Extracts perplexity vs token curves for each training run\n",
    "4. **Hyperparameter Optimization**: Identifies optimal learning rates and batch sizes for each configuration\n",
    "5. **Comparative Analysis**: Groups results by base model configuration and scaling multipliers\n",
    "\n",
    "## Visualizations\n",
    "\n",
    "The notebook generates comparative plots showing:\n",
    "- **Best Final Perplexity vs Learning Rate**: Optimal learning rate identification for each scaling configuration\n",
    "- **Best Final Perplexity vs Batch Size**: Optimal batch size identification for each scaling configuration\n",
    "\n",
    "Results are grouped by base model configuration (d_model_base, number of experts, normalization settings) with different lines representing various depth/width multiplier combinations.\n",
    "\n",
    "## Key Metrics\n",
    "\n",
    "- **Final Perplexity**: Primary performance metric measured after training completion\n",
    "- **Total Tokens**: Training data volume (filters for runs with ‚â•1B tokens)\n",
    "- **Parameter Counts**: Breakdown of trainable, expert, and non-expert parameters\n",
    "- **Hyperparameter Ranges**: Learning rates, batch sizes, and scaling multipliers tested\n",
    "\n",
    "This analysis supports research into optimal scaling strategies for MoE models and provides insights into the ŒºP Complete-P methodology's effectiveness across different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import operator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from fedmoe_plots.data_analysis import ColumnNotFoundError, get_perplexity_versus_tokens\n",
    "from fedmoe_plots.parameter_counting import compute_parameter_counts\n",
    "from fedmoe_plots.plotting_utils import configure_logging_for_jupyter\n",
    "from fedmoe_plots.wandb_utils import (\n",
    "    download_wandb_whole_history,\n",
    "    get_clientrun_property_from_config,\n",
    "    get_run_uuid_from_config,\n",
    ")\n",
    "\n",
    "configure_logging_for_jupyter()\n",
    "\n",
    "log = logging.getLogger(\"mup_completep_scaling.ipynb\")\n",
    "\n",
    "EXCLUDE_INCOMPLETE_RUNS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70234a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOUR_EXPERTS_RUNS = [\n",
    "    \"(^tune-sigma_moe_4e_44m)\",\n",
    "    \"(^tune-deptoe_4e_256)\",\n",
    "    \"(^deptoe_4e_44m_mpt-base)\",\n",
    "    \"(^tune-deptoe_4e_128)\",\n",
    "    \"(^tune-deptoe_4e_depth)\",\n",
    "    \"(^tune-deptoe_4e_max)\",\n",
    "    \"(^tune-deptoe_4e_width)\",\n",
    "]\n",
    "\n",
    "EIGHT_EXPERTS_RUNS = [\n",
    "    \"(^tune-deptoe_8e_128)\",\n",
    "    \"(^tune-deptoe_8e_256)\",\n",
    "    \"(^tune-deptoe_8e_base)\",\n",
    "    \"(^tune-deptoe_8e_mpt-base)\",\n",
    "]\n",
    "\n",
    "RUNS_REGEX = [\n",
    "    \"(^tune-mpt_18m)\",\n",
    "    \"(^tune-mpt_1B)\",\n",
    "    \"(^tune-mpt_200m)\",\n",
    "    \"(^tune-mpt_3m)\",\n",
    "    \"(^tune-peri_mpt_18m)\",\n",
    "    \"(^tune-peri_mpt_200m)\",\n",
    "    \"(^tune-peri_mpt_3m)\",\n",
    "    *FOUR_EXPERTS_RUNS,\n",
    "    *EIGHT_EXPERTS_RUNS,\n",
    "]\n",
    "RUNS_NO_PERI_LN = [\n",
    "    \"(^tune-mpt_3m)\",\n",
    "    \"(^tune-mpt_18m)\",\n",
    "    \"(^tune-mpt_200m)\",\n",
    "    \"(^tune-mpt_1B)\",\n",
    "]\n",
    "RUNS_W_PERI_LN = set(RUNS_REGEX) - set(RUNS_NO_PERI_LN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api(timeout=100)\n",
    "full_runs_regex = \"|\".join(RUNS_REGEX)\n",
    "runs = api.runs(\n",
    "    path=\"camlsys/photon\",\n",
    "    filters={\"display_name\": {\"$regex\": f\"{full_runs_regex}\"}},\n",
    ")\n",
    "log.info(\"Found %d runs matching the regex: %s\", len(runs), full_runs_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_n_experts(config: dict) -> int:\n",
    "    try:\n",
    "        n_experts = config[\"llm_config\"][\"model\"][\"ffn_config\"][\"ff_n_experts\"]\n",
    "        assert n_experts is not None, \"ff_n_experts should not be None\"\n",
    "        assert isinstance(\n",
    "            n_experts,\n",
    "            int,\n",
    "        ), f\"ff_n_experts should be an integer and not of type {type(n_experts)}\"\n",
    "\n",
    "    except KeyError:\n",
    "        return 1\n",
    "    else:\n",
    "        return n_experts\n",
    "\n",
    "\n",
    "def _get_d_model_base(config: dict) -> int:\n",
    "    d_model = config[\"llm_config\"][\"model\"][\"d_model\"]\n",
    "    assert d_model is not None, \"d_model should not be None\"\n",
    "    assert isinstance(d_model, int), \"d_model should be an integer\"\n",
    "    width_multiplier = config[\"llm_config\"][\"model\"][\"mup_config\"][\n",
    "        \"mup_width_multiplier\"\n",
    "    ]\n",
    "    assert width_multiplier is not None, \"mup_width_multiplier should not be None\"\n",
    "    assert isinstance(\n",
    "        width_multiplier,\n",
    "        int,\n",
    "    ), \"mup_width_multiplier should be an integer\"\n",
    "    return d_model // width_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b0f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_depth_multiplier = {\n",
    "    get_clientrun_property_from_config(\n",
    "        run,\n",
    "        get_property_fn=lambda config: config[\"llm_config\"][\"model\"][\"mup_config\"][\n",
    "            \"completep_depth_multiplier\"\n",
    "        ],\n",
    "    )\n",
    "    for run in runs\n",
    "}\n",
    "unique_width_multiplier = {\n",
    "    get_clientrun_property_from_config(\n",
    "        run,\n",
    "        get_property_fn=lambda config: config[\"llm_config\"][\"model\"][\"mup_config\"][\n",
    "            \"mup_width_multiplier\"\n",
    "        ],\n",
    "    )\n",
    "    for run in runs\n",
    "}\n",
    "unique_lr = {\n",
    "    get_clientrun_property_from_config(\n",
    "        run,\n",
    "        get_property_fn=lambda config: config[\"llm_config\"][\"optimizer\"][\"lr\"],\n",
    "    )\n",
    "    for run in runs\n",
    "}\n",
    "unique_batch_size = {\n",
    "    get_clientrun_property_from_config(\n",
    "        run,\n",
    "        get_property_fn=lambda config: config[\"llm_config\"][\"global_train_batch_size\"],\n",
    "    )\n",
    "    for run in runs\n",
    "}\n",
    "unique_n_experts = {\n",
    "    get_clientrun_property_from_config(\n",
    "        run,\n",
    "        get_property_fn=_get_n_experts,\n",
    "    )\n",
    "    for run in runs\n",
    "}\n",
    "unique_peri_ln = {\n",
    "    get_clientrun_property_from_config(\n",
    "        run,\n",
    "        get_property_fn=lambda config: config[\"llm_config\"][\"model\"][\"use_peri_norm\"],\n",
    "    )\n",
    "    for run in runs\n",
    "}\n",
    "unique_embedding_ln = {\n",
    "    get_clientrun_property_from_config(\n",
    "        run,\n",
    "        get_property_fn=lambda config: config[\"llm_config\"][\"model\"][\n",
    "            \"use_embedding_norm\"\n",
    "        ],\n",
    "    )\n",
    "    for run in runs\n",
    "}\n",
    "unique_no_bias = {\n",
    "    get_clientrun_property_from_config(\n",
    "        run,\n",
    "        get_property_fn=lambda config: config[\"llm_config\"][\"model\"][\"no_bias\"],\n",
    "    )\n",
    "    for run in runs\n",
    "}\n",
    "unique_d_model_base = {\n",
    "    get_clientrun_property_from_config(\n",
    "        run,\n",
    "        get_property_fn=_get_d_model_base,\n",
    "    )\n",
    "    for run in runs\n",
    "}\n",
    "log.info(\n",
    "    \"Unique depth multipliers: %s\",\n",
    "    \", \".join(str(dm) for dm in unique_depth_multiplier if dm is not None),\n",
    ")\n",
    "log.info(\n",
    "    \"Unique width multipliers: %s\",\n",
    "    \", \".join(str(wm) for wm in unique_width_multiplier if wm is not None),\n",
    ")\n",
    "log.info(\n",
    "    \"Unique learning rates: %s\",\n",
    "    \", \".join(str(lr) for lr in unique_lr if lr is not None),\n",
    ")\n",
    "log.info(\n",
    "    \"Unique batch sizes: %s\",\n",
    "    \", \".join(str(bs) for bs in unique_batch_size if bs is not None),\n",
    ")\n",
    "log.info(\n",
    "    \"Unique number of experts: %s\",\n",
    "    \", \".join(str(ne) for ne in unique_n_experts if ne is not None),\n",
    ")\n",
    "log.info(\n",
    "    \"Unique use_peri_norm: %s\",\n",
    "    \", \".join(str(p) for p in unique_peri_ln if p is not None),\n",
    ")\n",
    "log.info(\n",
    "    \"Unique use_embedding_norm: %s\",\n",
    "    \", \".join(str(en) for en in unique_embedding_ln if en is not None),\n",
    ")\n",
    "log.info(\n",
    "    \"Unique no_bias: %s\",\n",
    "    \", \".join(str(nb) for nb in unique_no_bias if nb is not None),\n",
    ")\n",
    "log.info(\n",
    "    \"Unique d_model_base: %s\",\n",
    "    \", \".join(str(dm) for dm in unique_d_model_base if dm is not None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d187338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection and processing for expert density analysis\n",
    "\n",
    "log.info(\"üîÑ Starting data collection and processing...\")\n",
    "log.info(\"Found %s runs to process\", len(runs))\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for i, run in enumerate(runs):\n",
    "    log.info(\n",
    "        \"üìä Processing run %s/%s: %s\",\n",
    "        i + 1,\n",
    "        len(runs),\n",
    "        get_run_uuid_from_config(run),\n",
    "    )\n",
    "    try:\n",
    "        # Extract configuration parameters (depth multiplier, width multiplier,\n",
    "        # learning rate, batch size, number of experts, hidden dimension of the model,\n",
    "        # run UUID)\n",
    "        run_uuid = get_run_uuid_from_config(run)\n",
    "        depth_multiplier = get_clientrun_property_from_config(\n",
    "            run,\n",
    "            get_property_fn=lambda config: config[\"llm_config\"][\"model\"][\"mup_config\"][\n",
    "                \"completep_depth_multiplier\"\n",
    "            ],\n",
    "        )\n",
    "        width_multiplier = get_clientrun_property_from_config(\n",
    "            run,\n",
    "            get_property_fn=lambda config: config[\"llm_config\"][\"model\"][\"mup_config\"][\n",
    "                \"mup_width_multiplier\"\n",
    "            ],\n",
    "        )\n",
    "        learning_rate = get_clientrun_property_from_config(\n",
    "            run,\n",
    "            get_property_fn=lambda config: config[\"llm_config\"][\"optimizer\"][\"lr\"],\n",
    "        )\n",
    "        global_train_batch_size = get_clientrun_property_from_config(\n",
    "            run,\n",
    "            get_property_fn=lambda config: config[\"llm_config\"][\n",
    "                \"global_train_batch_size\"\n",
    "            ],\n",
    "        )\n",
    "        n_total_experts = get_clientrun_property_from_config(\n",
    "            run,\n",
    "            get_property_fn=_get_n_experts,\n",
    "        )\n",
    "        d_model_base = get_clientrun_property_from_config(\n",
    "            run,\n",
    "            get_property_fn=_get_d_model_base,\n",
    "        )\n",
    "        peri_ln = get_clientrun_property_from_config(\n",
    "            run,\n",
    "            get_property_fn=lambda config: config[\"llm_config\"][\"model\"][\n",
    "                \"use_peri_norm\"\n",
    "            ],\n",
    "        )\n",
    "        embedding_ln = get_clientrun_property_from_config(\n",
    "            run,\n",
    "            get_property_fn=lambda config: config[\"llm_config\"][\"model\"][\n",
    "                \"use_embedding_norm\"\n",
    "            ],\n",
    "        )\n",
    "        no_bias = get_clientrun_property_from_config(\n",
    "            run,\n",
    "            get_property_fn=lambda config: config[\"llm_config\"][\"model\"][\"no_bias\"],\n",
    "        )\n",
    "\n",
    "        # Calculate derived metrics (number of total trainable parameters, number of\n",
    "        # non-expert parameters, number of expert parameters, number of non embedding\n",
    "        # parameters, number of embedding parameters)\n",
    "        run_config = run.config\n",
    "        assert run_config is not None, \"Run config must not be None\"\n",
    "        assert isinstance(\n",
    "            run_config,\n",
    "            dict,\n",
    "        ), f\"Run config must be a dictionary, not {type(run_config)}\"\n",
    "        llm_config = run_config.get(\"llm_config\", {})\n",
    "        assert llm_config is not None, \"Model config must not be None\"\n",
    "        assert isinstance(\n",
    "            llm_config,\n",
    "            dict,\n",
    "        ), f\"Model config must be a dictionary, not {type(llm_config)}\"\n",
    "        parameter_counts = compute_parameter_counts(llm_config)\n",
    "        n_trainable_parameters = parameter_counts.n_trainable\n",
    "        n_non_expert_parameters = parameter_counts.n_non_experts\n",
    "        n_expert_parameters = parameter_counts.n_experts\n",
    "        n_non_embedding_parameters = parameter_counts.n_non_embedding\n",
    "        n_embedding_parameters = parameter_counts.n_embedding\n",
    "\n",
    "        # Download and process data\n",
    "        log.info(\"üîΩ Downloading data for %s...\", run_uuid)\n",
    "\n",
    "        try:\n",
    "            # Download photon metrics for the run\n",
    "            assert run_uuid is not None, \"Run UUID must not be None\"\n",
    "            assert isinstance(\n",
    "                run_uuid,\n",
    "                str,\n",
    "            ), f\"Run UUID must be a string not a {type(run_uuid)}\"\n",
    "            run_df = download_wandb_whole_history(\n",
    "                run=run,\n",
    "            )\n",
    "\n",
    "            # Try to get perplexity vs tokens data\n",
    "            tokens, perplexity = get_perplexity_versus_tokens(\n",
    "                client_metrics_df=run_df,\n",
    "                n_clients_per_round=1,\n",
    "            )\n",
    "\n",
    "            if len(tokens) == 0 or len(perplexity) == 0:\n",
    "                log.warning(\"‚ùå Empty data arrays for %s\", run_uuid)\n",
    "                continue\n",
    "\n",
    "            # Calculate metrics\n",
    "            final_perplexity = (\n",
    "                perplexity.iloc[-1] if len(perplexity) > 0 else float(\"nan\")\n",
    "            )\n",
    "            total_tokens = tokens.iloc[-1] if len(tokens) > 0 else 0\n",
    "            n_data_points = len(tokens)\n",
    "\n",
    "            log.info(\n",
    "                (\n",
    "                    \"   ‚úÖ Data processed: %d points,\"\n",
    "                    \" Final perplexity: %.4f, Total tokens: %.0f\"\n",
    "                ),\n",
    "                n_data_points,\n",
    "                final_perplexity,\n",
    "                total_tokens,\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            results_list.append(\n",
    "                {\n",
    "                    \"run_uuid\": str(run_uuid),\n",
    "                    \"d_model_base\": d_model_base,\n",
    "                    \"n_total_experts\": n_total_experts,\n",
    "                    \"depth_multiplier\": depth_multiplier,\n",
    "                    \"width_multiplier\": width_multiplier,\n",
    "                    \"peri_ln\": peri_ln,\n",
    "                    \"embedding_ln\": embedding_ln,\n",
    "                    \"no_bias\": no_bias,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"global_train_batch_size\": global_train_batch_size,\n",
    "                    \"n_trainable_parameters\": n_trainable_parameters,\n",
    "                    \"n_non_expert_parameters\": n_non_expert_parameters,\n",
    "                    \"n_expert_parameters\": n_expert_parameters,\n",
    "                    \"n_non_embedding_parameters\": n_non_embedding_parameters,\n",
    "                    \"n_embedding_parameters\": n_embedding_parameters,\n",
    "                    \"final_perplexity\": final_perplexity,\n",
    "                    \"total_tokens\": total_tokens,\n",
    "                    \"n_data_points\": n_data_points,\n",
    "                    \"tokens\": (\n",
    "                        tokens.tolist() if hasattr(tokens, \"tolist\") else list(tokens)\n",
    "                    ),\n",
    "                    \"perplexity\": (\n",
    "                        perplexity.tolist()\n",
    "                        if hasattr(perplexity, \"tolist\")\n",
    "                        else list(perplexity)\n",
    "                    ),\n",
    "                },\n",
    "            )\n",
    "\n",
    "        except ColumnNotFoundError:\n",
    "            log.exception(\n",
    "                \"   ‚ö†Ô∏è Column not found in client metrics DataFrame for %s. \"\n",
    "                \"We assume this run crashed and doesn't have the expected data.\",\n",
    "                run_uuid,\n",
    "                stack_info=True,\n",
    "            )\n",
    "\n",
    "            # Remove this run from WandB runs to avoid further processing\n",
    "            if run.state != \"running\":\n",
    "                log.info(\n",
    "                    \"Removing run %s with display name %s.\",\n",
    "                    run.id,\n",
    "                    run.display_name,\n",
    "                )\n",
    "                run.delete()\n",
    "            continue\n",
    "\n",
    "        except Exception:\n",
    "            log.exception(\"   ‚ùå Error processing %s\", run_uuid, stack_info=True)\n",
    "            continue\n",
    "\n",
    "    except Exception:\n",
    "        assert run is not None, \"Run must not be None\"\n",
    "        log.exception(\n",
    "            \"   ‚ùå Error extracting config for run %s\",\n",
    "            run.name,\n",
    "            stack_info=True,\n",
    "        )\n",
    "        continue\n",
    "\n",
    "log.info(\"\\n‚úÖ Data collection completed!\")\n",
    "log.info(\"Successfully processed %d out of %d runs\", len(results_list), len(runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e9e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"\\n=== Summary ===\")\n",
    "log.info(\n",
    "    \"Successfully processed %d out of %d runs\",\n",
    "    len(results_list),\n",
    "    len(runs),\n",
    ")\n",
    "\n",
    "incomplete_runs = []\n",
    "complete_runs = []\n",
    "\n",
    "# Check for runs that didn't reach 1 billion tokens and log warnings\n",
    "if results_list:\n",
    "    log.info(\"\\n‚ö†Ô∏è  TOKEN COUNT ANALYSIS ‚ö†Ô∏è\")\n",
    "    log.info(\"=\" * 60)\n",
    "\n",
    "    billion_tokens = 1e9\n",
    "\n",
    "    for result in results_list:\n",
    "        if result[\"total_tokens\"] < billion_tokens:\n",
    "            incomplete_runs.append(result)\n",
    "        else:\n",
    "            complete_runs.append(result)\n",
    "\n",
    "    if incomplete_runs:\n",
    "        log.warning(\n",
    "            \"üî¥ WARNING: %d run(s) did NOT reach 1 billion tokens:\",\n",
    "            len(incomplete_runs),\n",
    "        )\n",
    "        log.info(\"-\" * 60)\n",
    "\n",
    "        for i, result in enumerate(incomplete_runs, 1):\n",
    "            log.info(\"\\n%s. Run UUID: %s\", i, result[\"run_uuid\"])\n",
    "            log.info(\n",
    "                \"   Total Tokens: %s (%.3fB)\",\n",
    "                format(result[\"total_tokens\"], \",\"),\n",
    "                result[\"total_tokens\"] / 1e9,\n",
    "            )\n",
    "            log.info(\n",
    "                \"   Completion: %.1f%% of 1B tokens\",\n",
    "                result[\"total_tokens\"] / billion_tokens * 100,\n",
    "            )\n",
    "            log.info(\"   üìã Full Configuration:\")\n",
    "            log.info(\"      ‚Ä¢ Run UUID: %s\", result[\"run_uuid\"])\n",
    "            log.info(\n",
    "                \"      ‚Ä¢ Hidden dimension of the base model: %s\",\n",
    "                result[\"d_model_base\"],\n",
    "            )\n",
    "            log.info(\"      ‚Ä¢ Use Peri-LN: %s\", result[\"peri_ln\"])\n",
    "            log.info(\"      ‚Ä¢ Use Embedding-LN: %s\", result[\"embedding_ln\"])\n",
    "            log.info(\"      ‚Ä¢ No Bias: %s\", result[\"no_bias\"])\n",
    "            log.info(\"      ‚Ä¢ Total Experts: %s\", result[\"n_total_experts\"])\n",
    "            log.info(\"      ‚Ä¢ Depth Multiplier: %s\", result[\"depth_multiplier\"])\n",
    "            log.info(\"      ‚Ä¢ Width Multiplier: %s\", result[\"width_multiplier\"])\n",
    "            log.info(\"      ‚Ä¢ Learning Rate: %s\", result[\"learning_rate\"])\n",
    "            log.info(\n",
    "                \"      ‚Ä¢ Global Train Batch Size: %s\",\n",
    "                result[\"global_train_batch_size\"],\n",
    "            )\n",
    "            log.info(\n",
    "                \"      ‚Ä¢ Trainable Parameters: %s\",\n",
    "                format(result[\"n_trainable_parameters\"], \",\"),\n",
    "            )\n",
    "            log.info(\n",
    "                \"      ‚Ä¢ Non-Expert Parameters: %s\",\n",
    "                format(result[\"n_non_expert_parameters\"], \",\"),\n",
    "            )\n",
    "            log.info(\n",
    "                \"      ‚Ä¢ Expert Parameters: %s\",\n",
    "                format(result[\"n_expert_parameters\"], \",\"),\n",
    "            )\n",
    "            log.info(\"      ‚Ä¢ Final Perplexity: %.4f\", result[\"final_perplexity\"])\n",
    "            log.info(\"      ‚Ä¢ Data Points: %s\", result[\"n_data_points\"])\n",
    "\n",
    "    if complete_runs:\n",
    "        log.info(\n",
    "            \"‚úÖ %d run(s) successfully reached 1+ billion tokens:\",\n",
    "            len(complete_runs),\n",
    "        )\n",
    "        for result in complete_runs:\n",
    "            log.info(\n",
    "                \"   ‚Ä¢ %s: %d tokens (%.3fB)\",\n",
    "                result[\"run_uuid\"],\n",
    "                result[\"total_tokens\"],\n",
    "                result[\"total_tokens\"] / 1e9,\n",
    "            )\n",
    "\n",
    "    log.info(\"\\nüìä Token Count Summary:\")\n",
    "    log.info(\"   ‚Ä¢ Complete runs (‚â•1B tokens): %d\", len(complete_runs))\n",
    "    log.info(\"   ‚Ä¢ Incomplete runs (<1B tokens): %d\", len(incomplete_runs))\n",
    "    if results_list:\n",
    "        avg_tokens = sum(r[\"total_tokens\"] for r in results_list) / len(results_list)\n",
    "        log.info(\n",
    "            \"   ‚Ä¢ Average tokens across all runs: %.0f (%.3fB)\",\n",
    "            avg_tokens,\n",
    "            avg_tokens / 1e9,\n",
    "        )\n",
    "\n",
    "this_cell_results_list = results_list\n",
    "if EXCLUDE_INCOMPLETE_RUNS:\n",
    "    this_cell_results_list = complete_runs\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "if this_cell_results_list:\n",
    "    results_df = pd.DataFrame(this_cell_results_list)\n",
    "    log.info(\"\\nResults DataFrame shape: %s\", results_df.shape)\n",
    "    log.info(\"\\nConfiguration summary:\")\n",
    "    summary_cols = [\n",
    "        \"run_uuid\",\n",
    "        \"d_model_base\",\n",
    "        \"peri_ln\",\n",
    "        \"embedding_ln\",\n",
    "        \"no_bias\",\n",
    "        \"n_total_experts\",\n",
    "        \"depth_multiplier\",\n",
    "        \"width_multiplier\",\n",
    "        \"learning_rate\",\n",
    "        \"global_train_batch_size\",\n",
    "        \"final_perplexity\",\n",
    "        \"n_trainable_parameters\",\n",
    "    ]\n",
    "    log.info(results_df[summary_cols].to_string(index=False, float_format=\"%.4f\"))\n",
    "else:\n",
    "    log.info(\"No results to analyze\")\n",
    "    results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the results by base configuration (d_model, n_experts) and then by\n",
    "# depth and width multiplier. For each group, plot the best final\n",
    "# perplexity versus the learning rate and the final perplexity versus the batch size\n",
    "# drawing a line for each multipliers configuration in the group.\n",
    "\n",
    "if results_df.empty:\n",
    "    log.warning(\"No results to analyze - results_df is empty\")\n",
    "else:\n",
    "    log.info(\"üîç Starting analysis of grouped results...\")\n",
    "\n",
    "    # Group by base configuration (d_model, n_total_experts)\n",
    "    base_groups = results_df.groupby(\n",
    "        [\n",
    "            \"d_model_base\",\n",
    "            \"n_total_experts\",\n",
    "            \"peri_ln\",\n",
    "            \"embedding_ln\",\n",
    "            \"no_bias\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    log.info(\"Found %d base groups\", len(base_groups))\n",
    "\n",
    "    # Statistical significance analysis across all data\n",
    "    log.info(\"\\nüìä STATISTICAL SIGNIFICANCE ANALYSIS:\")\n",
    "    log.info(\"=\" * 60)\n",
    "\n",
    "    # Check for duplicate configurations across the entire dataset\n",
    "    all_config_groups = results_df.groupby(\n",
    "        [\n",
    "            \"d_model_base\",\n",
    "            \"n_total_experts\",\n",
    "            \"peri_ln\",\n",
    "            \"embedding_ln\",\n",
    "            \"no_bias\",\n",
    "            \"depth_multiplier\",\n",
    "            \"width_multiplier\",\n",
    "            \"learning_rate\",\n",
    "            \"global_train_batch_size\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    total_duplicate_configs = 0\n",
    "    total_unique_configs = len(all_config_groups)\n",
    "\n",
    "    for _config, config_df in all_config_groups:\n",
    "        if len(config_df) > 1:\n",
    "            total_duplicate_configs += 1\n",
    "\n",
    "    log.info(\"Total unique configurations: %d\", total_unique_configs)\n",
    "    log.info(\n",
    "        \"Configurations with multiple runs: %d (%.1f%%)\",\n",
    "        total_duplicate_configs,\n",
    "        (\n",
    "            total_duplicate_configs / total_unique_configs * 100\n",
    "            if total_unique_configs > 0\n",
    "            else 0\n",
    "        ),\n",
    "    )\n",
    "    log.info(\n",
    "        \"Single-run configurations: %d (%.1f%%)\",\n",
    "        total_unique_configs - total_duplicate_configs,\n",
    "        (\n",
    "            (total_unique_configs - total_duplicate_configs)\n",
    "            / total_unique_configs\n",
    "            * 100\n",
    "            if total_unique_configs > 0\n",
    "            else 0\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if total_duplicate_configs > 0:\n",
    "        log.info(\"‚úÖ Statistical analysis will show mean ¬± std for multiple runs\")\n",
    "        log.info(\"   Error bars represent standard deviation across multiple runs\")\n",
    "    else:\n",
    "        log.info(\"   All configurations are single runs - no error bars will be shown\")\n",
    "    log.info(\"=\" * 60)\n",
    "\n",
    "    # Enlarge plots horizontally - increase width multiplier\n",
    "    fig, axes = plt.subplots(2, len(base_groups), figsize=(8 * len(base_groups), 12))\n",
    "\n",
    "    # Ensure axes is always 2D for consistent indexing\n",
    "    if len(base_groups) == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "\n",
    "    for group_idx, (\n",
    "        (d_model_base_val, n_experts_val, peri_ln_val, embedding_ln_val, no_bias_val),\n",
    "        base_group_df,\n",
    "    ) in enumerate(\n",
    "        base_groups,\n",
    "    ):\n",
    "        log.info(\n",
    "            (\n",
    "                \"üìä Processing base configuration: d_model_base=%s, n_experts=%s,\"\n",
    "                \" peri_ln=%s, embedding_ln=%s, no_bias=%s\"\n",
    "            ),\n",
    "            d_model_base_val,\n",
    "            n_experts_val,\n",
    "            peri_ln_val,\n",
    "            embedding_ln_val,\n",
    "            no_bias_val,\n",
    "        )\n",
    "\n",
    "        # Exclude outliers using IQR method\n",
    "        Q1 = base_group_df[\"final_perplexity\"].quantile(0.25)\n",
    "        Q3 = base_group_df[\"final_perplexity\"].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Filter out outliers\n",
    "        base_group_df_filtered = base_group_df[\n",
    "            (base_group_df[\"final_perplexity\"] >= lower_bound)\n",
    "            & (base_group_df[\"final_perplexity\"] <= upper_bound)\n",
    "        ]\n",
    "\n",
    "        log.info(\n",
    "            \"   Excluded %d outliers (%.1f%% of data)\",\n",
    "            len(base_group_df) - len(base_group_df_filtered),\n",
    "            (len(base_group_df) - len(base_group_df_filtered))\n",
    "            / len(base_group_df)\n",
    "            * 100,\n",
    "        )\n",
    "\n",
    "        # Group by multipliers configuration within this base group\n",
    "        multiplier_groups = base_group_df_filtered.groupby(\n",
    "            [\"depth_multiplier\", \"width_multiplier\"],\n",
    "        )\n",
    "\n",
    "        log.info(\"   Found %d multiplier configurations\", len(multiplier_groups))\n",
    "\n",
    "        # Plot 1: Best final perplexity vs learning rate\n",
    "        ax1 = axes[0, group_idx]\n",
    "\n",
    "        # Plot 2: Final perplexity vs batch size\n",
    "        ax2 = axes[1, group_idx]\n",
    "\n",
    "        colors = plt.cm.tab10(  # pyright: ignore[reportAttributeAccessIssue]\n",
    "            np.linspace(0, 1, len(multiplier_groups)),\n",
    "        )\n",
    "\n",
    "        # Track optimal values for vertical lines\n",
    "        optimal_lr_values = {}\n",
    "        optimal_bs_values = {}\n",
    "\n",
    "        for color_idx, ((depth_mult, width_mult), mult_group_df) in enumerate(\n",
    "            multiplier_groups,\n",
    "        ):\n",
    "            label = f\"depth x {depth_mult}, width x {width_mult}\"\n",
    "            color = colors[color_idx]\n",
    "\n",
    "            # For perplexity vs learning rate: group by learning rate and calculate\n",
    "            # mean and std for statistical significance\n",
    "            lr_groups = mult_group_df.groupby(\"learning_rate\")\n",
    "            lr_values = []\n",
    "            mean_perplexities_lr = []\n",
    "            std_perplexities_lr = []\n",
    "\n",
    "            for lr, lr_group in lr_groups:\n",
    "                perp_values = lr_group[\"final_perplexity\"]\n",
    "                mean_perp = perp_values.mean()\n",
    "                std_perp = perp_values.std() if len(perp_values) > 1 else 0.0\n",
    "\n",
    "                lr_values.append(lr)\n",
    "                mean_perplexities_lr.append(mean_perp)\n",
    "                std_perplexities_lr.append(std_perp)\n",
    "\n",
    "                # Log information about multiple runs for the same configuration\n",
    "                if len(perp_values) > 1:\n",
    "                    log.info(\n",
    "                        \"      LR %s: %d runs, mean=%.4f, std=%.4f\",\n",
    "                        lr,\n",
    "                        len(perp_values),\n",
    "                        mean_perp,\n",
    "                        std_perp,\n",
    "                    )\n",
    "\n",
    "            # Sort by learning rate for proper line plotting\n",
    "            lr_perp_pairs = sorted(\n",
    "                zip(lr_values, mean_perplexities_lr, std_perplexities_lr, strict=True),\n",
    "            )\n",
    "            lr_values, mean_perplexities_lr, std_perplexities_lr = (\n",
    "                zip(*lr_perp_pairs, strict=True) if lr_perp_pairs else ([], [], [])\n",
    "            )\n",
    "\n",
    "            # Plot learning rate vs mean perplexity with error bars\n",
    "            if lr_values:\n",
    "                ax1.errorbar(\n",
    "                    lr_values,\n",
    "                    mean_perplexities_lr,\n",
    "                    yerr=std_perplexities_lr,\n",
    "                    fmt=\"o-\",\n",
    "                    color=color,\n",
    "                    label=label,\n",
    "                    linewidth=2,\n",
    "                    markersize=6,\n",
    "                    capsize=5,\n",
    "                    capthick=1.5,\n",
    "                )\n",
    "\n",
    "                # Find optimal learning rate for this multiplier group (lowest mean)\n",
    "                min_perp_idx = np.argmin(mean_perplexities_lr)\n",
    "                optimal_lr = lr_values[min_perp_idx]\n",
    "                optimal_lr_values[depth_mult, width_mult] = (optimal_lr, color)\n",
    "\n",
    "            # For perplexity vs batch size: group by batch size and calculate\n",
    "            # mean and std for statistical significance\n",
    "            bs_groups = mult_group_df.groupby(\"global_train_batch_size\")\n",
    "            bs_values = []\n",
    "            mean_perplexities_bs = []\n",
    "            std_perplexities_bs = []\n",
    "\n",
    "            for bs, bs_group in bs_groups:\n",
    "                perp_values = bs_group[\"final_perplexity\"]\n",
    "                mean_perp = perp_values.mean()\n",
    "                std_perp = perp_values.std() if len(perp_values) > 1 else 0.0\n",
    "\n",
    "                bs_values.append(bs)\n",
    "                mean_perplexities_bs.append(mean_perp)\n",
    "                std_perplexities_bs.append(std_perp)\n",
    "\n",
    "                # Log information about multiple runs for the same configuration\n",
    "                if len(perp_values) > 1:\n",
    "                    log.info(\n",
    "                        \"      BS %s: %d runs, mean=%.4f, std=%.4f\",\n",
    "                        bs,\n",
    "                        len(perp_values),\n",
    "                        mean_perp,\n",
    "                        std_perp,\n",
    "                    )\n",
    "\n",
    "            # Sort by batch size for proper line plotting\n",
    "            bs_perp_pairs = sorted(\n",
    "                zip(bs_values, mean_perplexities_bs, std_perplexities_bs, strict=True),\n",
    "            )\n",
    "            bs_values, mean_perplexities_bs, std_perplexities_bs = (\n",
    "                zip(*bs_perp_pairs, strict=True) if bs_perp_pairs else ([], [], [])\n",
    "            )\n",
    "\n",
    "            # Plot batch size vs mean perplexity with error bars\n",
    "            if bs_values:\n",
    "                ax2.errorbar(\n",
    "                    bs_values,\n",
    "                    mean_perplexities_bs,\n",
    "                    yerr=std_perplexities_bs,\n",
    "                    fmt=\"s-\",\n",
    "                    color=color,\n",
    "                    label=label,\n",
    "                    linewidth=2,\n",
    "                    markersize=6,\n",
    "                    capsize=5,\n",
    "                    capthick=1.5,\n",
    "                )\n",
    "\n",
    "                # Find optimal batch size for this multiplier group (lowest mean)\n",
    "                min_perp_idx = np.argmin(mean_perplexities_bs)\n",
    "                optimal_bs = bs_values[min_perp_idx]\n",
    "                optimal_bs_values[depth_mult, width_mult] = (optimal_bs, color)\n",
    "\n",
    "        # Add vertical lines for optimal learning rates with labels\n",
    "        for optimal_lr, color in optimal_lr_values.values():\n",
    "            ax1.axvline(\n",
    "                x=optimal_lr,\n",
    "                color=color,\n",
    "                linestyle=\"--\",\n",
    "                alpha=0.7,\n",
    "                linewidth=1.5,\n",
    "            )\n",
    "            # Add label with learning rate value\n",
    "            ax1.text(\n",
    "                optimal_lr,\n",
    "                ax1.get_ylim()[1] * 0.95,\n",
    "                f\"{optimal_lr:.1e}\",\n",
    "                rotation=90,\n",
    "                verticalalignment=\"top\",\n",
    "                horizontalalignment=\"right\",\n",
    "                color=color,\n",
    "                fontsize=9,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "        # Add vertical lines for optimal batch sizes with labels\n",
    "        for optimal_bs, color in optimal_bs_values.values():\n",
    "            ax2.axvline(\n",
    "                x=optimal_bs,\n",
    "                color=color,\n",
    "                linestyle=\"--\",\n",
    "                alpha=0.7,\n",
    "                linewidth=1.5,\n",
    "            )\n",
    "            # Add label with batch size value\n",
    "            ax2.text(\n",
    "                optimal_bs,\n",
    "                ax2.get_ylim()[1] * 0.95,\n",
    "                f\"{optimal_bs}\",\n",
    "                rotation=90,\n",
    "                verticalalignment=\"top\",\n",
    "                horizontalalignment=\"right\",\n",
    "                color=color,\n",
    "                fontsize=9,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "        # Configure first subplot (learning rate vs perplexity)\n",
    "        ax1.set_xlabel(\"Learning Rate\", fontsize=12)\n",
    "        ax1.set_ylabel(\"Mean Final Perplexity ¬± Std\", fontsize=12)\n",
    "        ax1.set_title(\n",
    "            (\n",
    "                f\"Mean Perplexity vs Learning Rate\\n\"\n",
    "                f\"d_model_base={d_model_base_val}, n_experts={n_experts_val},\\n\"\n",
    "                f\" peri_ln={peri_ln_val}, embedding_ln={embedding_ln_val},\"\n",
    "                f\" no_bias={no_bias_val}\"\n",
    "            ),\n",
    "            fontsize=14,\n",
    "        )\n",
    "        ax1.set_xscale(\"log\")\n",
    "        ax1.grid(alpha=0.3)\n",
    "        ax1.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "        # Configure second subplot (batch size vs perplexity)\n",
    "        ax2.set_xlabel(\"Global Train Batch Size\", fontsize=12)\n",
    "        ax2.set_ylabel(\"Mean Final Perplexity ¬± Std\", fontsize=12)\n",
    "        ax2.set_title(\n",
    "            (\n",
    "                f\"Mean Perplexity vs Batch Size\\n\"\n",
    "                f\"d_model_base={d_model_base_val}, n_experts={n_experts_val}\"\n",
    "            ),\n",
    "            fontsize=14,\n",
    "        )\n",
    "        ax2.set_xscale(\"log\")\n",
    "        ax2.grid(alpha=0.3)\n",
    "        ax2.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "        log.info(\"   ‚úÖ Processed %s multiplier configurations\", len(multiplier_groups))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Additional analysis: Print summary statistics\n",
    "    log.info(\"\\nüìà SUMMARY STATISTICS BY BASE CONFIGURATION:\")\n",
    "    log.info(\"=\" * 70)\n",
    "\n",
    "    for (\n",
    "        d_model_base_val,\n",
    "        n_experts_val,\n",
    "        peri_ln_val,\n",
    "        embedding_ln_val,\n",
    "        no_bias_val,\n",
    "    ), base_group_df in base_groups:\n",
    "        log.info(\n",
    "            (\n",
    "                \"\\nüîπ Base config: d_model_base=%s, n_experts=%s,\"\n",
    "                \" peri_ln=%s, embedding_ln=%s, no_bias=%s\"\n",
    "            ),\n",
    "            d_model_base_val,\n",
    "            n_experts_val,\n",
    "            peri_ln_val,\n",
    "            embedding_ln_val,\n",
    "            no_bias_val,\n",
    "        )\n",
    "        log.info(\"   Total runs: %s\", len(base_group_df))\n",
    "\n",
    "        # Best overall perplexity in this base configuration (single best run)\n",
    "        best_run = base_group_df.loc[base_group_df[\"final_perplexity\"].idxmin()]\n",
    "        log.info(\"   Best single run perplexity: %.4f\", best_run[\"final_perplexity\"])\n",
    "        log.info(\n",
    "            \"   Best config: depth x %s, width x %s\",\n",
    "            best_run[\"depth_multiplier\"],\n",
    "            best_run[\"width_multiplier\"],\n",
    "        )\n",
    "        log.info(\"   Best LR: %s\", best_run[\"learning_rate\"])\n",
    "        log.info(\"   Best batch size: %s\", best_run[\"global_train_batch_size\"])\n",
    "\n",
    "        # Statistical analysis: check for configurations with multiple runs\n",
    "        config_groups = base_group_df.groupby(\n",
    "            [\n",
    "                \"depth_multiplier\",\n",
    "                \"width_multiplier\",\n",
    "                \"learning_rate\",\n",
    "                \"global_train_batch_size\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        multiple_run_configs = []\n",
    "        for config, config_df in config_groups:\n",
    "            if len(config_df) > 1:\n",
    "                depth_mult, width_mult, lr, bs = config\n",
    "                mean_perp = config_df[\"final_perplexity\"].mean()\n",
    "                std_perp = config_df[\"final_perplexity\"].std()\n",
    "                multiple_run_configs.append(\n",
    "                    {\n",
    "                        \"config\": config,\n",
    "                        \"n_runs\": len(config_df),\n",
    "                        \"mean_perp\": mean_perp,\n",
    "                        \"std_perp\": std_perp,\n",
    "                        \"min_perp\": config_df[\"final_perplexity\"].min(),\n",
    "                        \"max_perp\": config_df[\"final_perplexity\"].max(),\n",
    "                    },\n",
    "                )\n",
    "\n",
    "        if multiple_run_configs:\n",
    "            log.info(\n",
    "                \"   üìä Configurations with multiple runs (%d total):\",\n",
    "                len(multiple_run_configs),\n",
    "            )\n",
    "            for config_info in sorted(\n",
    "                multiple_run_configs,\n",
    "                key=operator.itemgetter(\"mean_perp\"),\n",
    "            ):\n",
    "                depth_mult, width_mult, lr, bs = config_info[\"config\"]\n",
    "                log.info(\n",
    "                    \"      ‚Ä¢ depth x %s, width x %s, LR=%s, BS=%s: %d runs\",\n",
    "                    depth_mult,\n",
    "                    width_mult,\n",
    "                    lr,\n",
    "                    bs,\n",
    "                    config_info[\"n_runs\"],\n",
    "                )\n",
    "                log.info(\n",
    "                    \"        Mean: %.4f ¬± %.4f, Range: [%.4f, %.4f]\",\n",
    "                    config_info[\"mean_perp\"],\n",
    "                    config_info[\"std_perp\"],\n",
    "                    config_info[\"min_perp\"],\n",
    "                    config_info[\"max_perp\"],\n",
    "                )\n",
    "        else:\n",
    "            log.info(\"       No configurations with multiple runs (all single runs)\")\n",
    "\n",
    "        # Best configuration by mean performance (for configs with multiple runs)\n",
    "        if multiple_run_configs:\n",
    "            best_config_info = min(\n",
    "                multiple_run_configs, key=operator.itemgetter(\"mean_perp\"),\n",
    "            )\n",
    "            depth_mult, width_mult, lr, bs = best_config_info[\"config\"]\n",
    "            log.info(\n",
    "                \"   üèÜ Best mean config: depth x %s, width x %s, LR=%s, BS=%s\",\n",
    "                depth_mult,\n",
    "                width_mult,\n",
    "                lr,\n",
    "                bs,\n",
    "            )\n",
    "            log.info(\n",
    "                \"      Mean perplexity: %.4f ¬± %.4f (%d runs)\",\n",
    "                best_config_info[\"mean_perp\"],\n",
    "                best_config_info[\"std_perp\"],\n",
    "                best_config_info[\"n_runs\"],\n",
    "            )\n",
    "\n",
    "        # Range of perplexities\n",
    "        perp_range = (\n",
    "            base_group_df[\"final_perplexity\"].max()\n",
    "            - base_group_df[\"final_perplexity\"].min()\n",
    "        )\n",
    "        log.info(\"   Perplexity range: %.4f\", perp_range)\n",
    "\n",
    "        # Multiplier configurations tested\n",
    "        multiplier_configs = base_group_df[\n",
    "            [\"depth_multiplier\", \"width_multiplier\"]\n",
    "        ].drop_duplicates()\n",
    "        log.info(\"   Multiplier configs tested: %s\", len(multiplier_configs))\n",
    "        for _, config in multiplier_configs.iterrows():\n",
    "            log.info(\n",
    "                \"      ‚Ä¢ depth x %s, width x %s\",\n",
    "                config[\"depth_multiplier\"],\n",
    "                config[\"width_multiplier\"],\n",
    "            )\n",
    "\n",
    "    log.info(\"\\n‚úÖ Analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe4e52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedmoe-plots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
